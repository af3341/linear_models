---
title: "cross_validation"
author: "Alana Ferris"
date: "2022-11-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelr)
library(mgcv)

set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# final project tips
- commit and then pull and then push --> as a way to figure out the conflicts and then put stuff back online to avoid major catastrophes 
- and stay in touch and be clear who is working on what at the same time 
- can change our proposal, it is not binding 

# hw 5 problem 2
- figure out what need to do for one city -- to get estimated proportions and confidence interval and iterate across cities 
- can always start with a for loop --> construct an output list then iterate across a collection of dataframes, each time 
- if have the inside of a for loop and inputting one dataframe each time, extracting function 
- i am mapping across this thing and doing this every time
- map inside mutate statement to create a tidy dataframe 

# hw 5 problem 3
- put the t.test in the function 
- go to simulations lecture and see what we can copy and paste 
- collection of paramters trying to vary between simulations and we want to keep track of iterations for each of those, want to map across sample size column the simulate mean and sd function 
-- this said given sample size with true mean and sd return the est. mean and sd 
-- do this a bunch, unnest, see dataframe 
-- this is the starting point
- not changing sample size but true mean 

```{r}
sample = rnorm(30, mean = 0)

test_results = t.test(sample)

test_results %>% 
  broom::tidy()

# now need to wrap this function up where can change the mean 

sim_t_test = function(true_mean){
 
   test_results = t.test(sample)
  
  test_results %>% 
  broom::tidy()

}

# then come back to the expand grid part of class notes, iterate across collection of true mean values 
# make sure to keep track of estimate and p value 

```

- why make simulation in the first place --> to avoid publication bias so you're  not just seeing significant ps every time, to see the truth better
- we can do a simulation to see how publication bias plays out in real life and why we need to be skeptical of reported results in the literature 

# Step 1 

```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - .3) ^ 2 + rnorm(100, 0, .3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point()
```
- next model to fit that tries to capture some degree of nonlinearity 
- bc linear model wont be complex enough
- smooth one has right amount of complexity and fit...?
- no easy test to decide which model

# Let's get this by hand 

- training data: 80% of data started with
- testing data is everything else 

```{r}
train_df = sample_n(nonlin_df, 80)

test_df = anti_join(nonlin_df, train_df, by = "id")
```

- `anti_join` anything that is in this dataframe that isn't in the other--> here anything that wasn't in training dataset 

```{r}
train_df %>% 
ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_point(data = test_df, color = "red")
  
```
## Let's try to fit three models on training data

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

- mgcv does smooth fits 

### results

- in `add_predictions` input the different models you coded above

```{r}
train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")
```

Let's make predictions and compute RMSEs

- add predictions to testing model

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

- RMSEs = prediction error so lower prediction error means prediction is more accurate 
- we want to test this multiple times to see if this is the "truth"

## Can we iterate?

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100)

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
```

- by default it should do 20% in testing dataset, 80% in training 
- pulling out first thing in dataframe and making it a list 
- in the end we are just trying to keep track of everything 

- in each training dataset want to fit linear model and get the RMSE..? root mean squared errors is am i geting good prediction accuracy
- in first mutate, want to convert resamples to tibbles(dataframes)
```{r}
cv_df = 
  crossv_mc(nonlin_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) 
```

- what need to do now is add a column to dataframe that contains result of fitting lin model to train test
then another and another fit to the train dataset 
- and do this by mapping over training dataset --> give me the first entry of the column and apply the function and so on
```{r}
cv_df = 
  crossv_mc(nonlin_df, 100) %>% #crossv_mc is doing training testing split
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% # this converts training testing dataset into tibble so we can work with it
  mutate(
    linear_fits = map(.x = train, ~lm(y ~ x, data = .x)) # in this case each of those datasets contains a y and an x and we want to fit linear model y against x using training dataset
  )
  
```
- put .x to refer to the column entries bc it originally thinks the first entry should be an equation 
-motivation for cross validation is model comparison/model selection --> some variety of predictors/tuning parameters that will effect quality of model youre picking
--could just be hypothesis test, other cases much less clear

-code we writing today 11/21 is going to be like other code we have written: a list column that contains dataframes, going to fit models going across dataframe and add to that RMSEs

## now we are going to compute RMSE for linear fit 
```{r}
cv_df = 
  crossv_mc(nonlin_df, 100) %>% #crossv_mc is doing training testing split
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% # this converts training testing dataset into tibble so we can work with it
  mutate(
    linear_fits = map(.x = train, ~lm(y ~ x, data = .x)),
    smooth_fits = map(.x = train, ~mgcv::gam(y ~ s(x), data = .x)),
    wiggly_fits = map(.x = train, ~mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))# in this case each of those datasets contains a y and an x and we want to fit linear model y against x using training dataset
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_fits, .y = test, ~rmse(model = .x, data = .y)),
    rmse_smooth_fits = map2_dbl(.x = smooth_fits, .y = test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(.x = wiggly_fits, .y = test, ~rmse(model = .x, data = .y))
  )
 
```
RMSE needs both the model and the dataset 
-map 2 giving us 2 things to map across, the linear fits and the testing datasets

# make a box plot 

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```
# example: child growth 

```{r}
growth_df = read_csv("./data/nepalese_children.csv") 

growth_df %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = .3)
```
## brief aside on piecewise linear models 

```{r}
growth_df =
  growth_df %>% 
  mutate(
    weight_pwl = (weight > 7) * (weight - 7) #anytime weight less than 7, get 0 
  )

pwl_model = 
  lm(armc ~ weight + weight_pwl, data = growth_df)

growth_df %>% 
  add_predictions(pwl_model) %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")
# this kind of tricks the model to mimic nonlinearity bc see a change in slope before 7 and after 7 for this example 
# can fit a model like this with changepoint fit to give reasonable interpretations between sides 
```
## continuing with child growth example

```{r}
linear_model = lm(armc ~ weight, data = growth_df)
pwl_model = lm(armc ~ weight + weight_pwl, data = growth_df)
smooth_model = mgcv::gam(armc ~s(weight), data = growth_df)

# now we are doing cross validation to see which fit is really the best 
```

### cross validation of child growth models

```{r}
cv_df = 
  crossv_mc(growth_df, 100) %>% #crossv_mc is doing training testing split given 1 dataset and do 80 20 training testing split (?)
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% # this converts training testing dataset into tibble so we can work with it
  mutate(
    linear_fits = map(.x = train, ~lm(armc ~ weight, data = .x)),
    pwl_fits =    map(.x = train, ~lm(armc ~ weight + weight_pwl, data = .x)),
    smooth_fits = map(.x = train, ~mgcv::gam(armc ~ s(weight), data = .x))# in this case each of those datasets contains a y and an x and we want to fit linear model y against x using training dataset
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_fits, .y = test, ~rmse(model = .x, data = .y)),
    rmse_pwl =    map2_dbl(.x = pwl_fits,    .y = test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(.x = smooth_fits, .y = test, ~rmse(model = .x, data = .y))
  )
```

### make a boxplot 

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```
-smooth is slightly better, if just looking to capture the nonlinearity and don't care about interpretation here go with this.
-pwl fit has 2 parameters, slope less than 7 and slope more than 7 --> has good balance of goodness of fit and giving interpretation that is easy. and here he just picked 7, this is a place where you choose the change point through cross validation in the real world 
-and linear clearly not doing well
-linear model is usually not a good fit, highly biased with low variance bc pretty much always going to fit the data the same way (not a lot of variance) but is going to be too simple most of the time to actually capture data 

# Bootstrapping notes
-everythign around hypothesis testing is based in repeated sampling 
-bootstrapping is sampling from your sample , always drawn with replacement 
-tool for statistical inference 

# homework 6 
-this should not be quite as bad 
-second problem revisits washington dataset 
--we did logistic regression under binary outcomes in linear models lecture which is what you need for first half of problem 2 
-problem 3 is comparing 3 linear models via cross validation, they give us 2 we need to come up with a 3rd 
